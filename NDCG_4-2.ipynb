{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Load the data from the pickled file\n",
        "file_path = '/content/drive/My Drive/synthetic_dataset/goal_set.p'\n",
        "with open(file_path, 'rb') as file:\n",
        "    consultation_data = pickle.load(file)\n",
        "\n",
        "test_data = consultation_data['test']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKe0j4dodyFo",
        "outputId": "5d98a252-00b9-48e2-fb90-befe50e74af6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Extract explicit and implicit symptoms along with their disease tags\n",
        "disease_tags = []\n",
        "explicit_symptoms_list = []\n",
        "implicit_symptoms_list = []\n",
        "\n",
        "for item in test_data:\n",
        "    disease_tags.append(item['disease_tag'])\n",
        "    explicit_symptoms = list(item['goal']['explicit_inform_slots'].keys())\n",
        "    implicit_symptoms = list(item['goal']['implicit_inform_slots'].keys())\n",
        "    explicit_symptoms_list.append(\", \".join(explicit_symptoms))\n",
        "    implicit_symptoms_list.append(\", \".join(implicit_symptoms))\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Disease Tag': disease_tags,\n",
        "    'Explicit Symptoms': explicit_symptoms_list,\n",
        "    'Implicit Symptoms': implicit_symptoms_list\n",
        "})\n",
        "\n",
        "# Define and train the models\n",
        "X = df['Explicit Symptoms'] + \" \" + df['Implicit Symptoms']\n",
        "y = df['Disease Tag']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'SVM': SVC(probability=True, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Define NDCG calculation functions\n",
        "def dcg_score(y_true, y_score, k=10):\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "    gain = 2 ** y_true - 1\n",
        "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
        "    return np.sum(gain / discounts)\n",
        "\n",
        "def ndcg_score(y_true, y_score, k=10):\n",
        "    best = dcg_score(y_true, y_true, k)\n",
        "    actual = dcg_score(y_true, y_score, k)\n",
        "    return actual / best\n",
        "\n",
        "# Train, predict and evaluate each model\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Calculate NDCG@10 and NDCG@5\n",
        "    y_test_bin = MultiLabelBinarizer().fit_transform([[label] for label in y_test])\n",
        "    y_score = model.predict_proba(X_test)\n",
        "    ndcg_scores_10 = [ndcg_score(y_test_bin[i], y_score[i], k=10) for i in range(len(y_test))]\n",
        "    ndcg_scores_5 = [ndcg_score(y_test_bin[i], y_score[i], k=5) for i in range(len(y_test))]\n",
        "    mean_ndcg_10 = np.mean(ndcg_scores_10)\n",
        "    mean_ndcg_5 = np.mean(ndcg_scores_5)\n",
        "\n",
        "    # Store results\n",
        "    results[model_name] = {\n",
        "        'Accuracy': accuracy,\n",
        "        'NDCG@10': mean_ndcg_10,\n",
        "        'NDCG@5': mean_ndcg_5\n",
        "    }\n",
        "\n",
        "# Display results\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"{model_name} - Accuracy: {metrics['Accuracy']:.2f}, NDCG@10: {metrics['NDCG@10']:.5f}, NDCG@5: {metrics['NDCG@5']:.5f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NBgIBSceFVD",
        "outputId": "bf849fcb-bc1a-4ede-aefe-5f6d1d7b5d7b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest - Accuracy: 0.75, NDCG@10: 0.86821, NDCG@5: 0.86349\n",
            "Logistic Regression - Accuracy: 0.78, NDCG@10: 0.89851, NDCG@5: 0.89337\n",
            "SVM - Accuracy: 0.73, NDCG@10: 0.88421, NDCG@5: 0.88001\n",
            "Gradient Boosting - Accuracy: 0.74, NDCG@10: 0.86196, NDCG@5: 0.85392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 랜덤 포레스트 (Random Forest)\n",
        "**장점**:\n",
        "- 강력한 성능: 다양한 데이터에서 좋은 성능을 보여줍니다.\n",
        "- 과적합 방지: 여러 결정 트리를 사용하여 과적합을 줄입니다.\n",
        "- 해석 가능성: 각 트리와 특성의 중요도를 파악할 수 있습니다.\n",
        "\n",
        "**단점**:\n",
        "- 느린 예측: 많은 트리를 사용하므로 예측 속도가 느릴 수 있습니다.\n",
        "- 메모리 사용량: 많은 트리를 저장해야 하므로 메모리 사용량이 높을 수 있습니다.\n",
        "\n",
        "**NDCG 개선 방법**:\n",
        "- 더 많은 트리 사용 (n_estimators 증가)\n",
        "- 최적의 트리 깊이 설정 (max_depth 조정)\n",
        "- 특성 중요도 분석을 통해 중요한 특성만 사용\n",
        "\n",
        "### 2. 로지스틱 회귀 (Logistic Regression)\n",
        "**장점**:\n",
        "- 간단하고 빠른 모델: 학습과 예측이 빠릅니다.\n",
        "- 해석 가능성: 모델이 설명하기 쉽습니다.\n",
        "- 과적합 방지: 규제(regularization)를 사용하여 과적합을 방지합니다.\n",
        "\n",
        "**단점**:\n",
        "- 선형성 가정: 특성과 라벨 간의 선형 관계를 가정합니다.\n",
        "- 비선형 문제에 적합하지 않음: 복잡한 패턴을 잘 포착하지 못할 수 있습니다.\n",
        "\n",
        "**NDCG 개선 방법**:\n",
        "- 규제 강도 조정 (C 매개변수 조정)\n",
        "- 다항 특성(polynomial features) 추가\n",
        "- 특성 선택(feature selection)을 통해 중요한 특성만 사용\n",
        "\n",
        "### 3. 서포트 벡터 머신 (SVM)\n",
        "**장점**:\n",
        "- 고차원 데이터에서 효과적: 복잡한 결정 경계를 잘 찾습니다.\n",
        "- 다양한 커널 함수 지원: 비선형 문제도 해결 가능\n",
        "\n",
        "**단점**:\n",
        "- 계산 비용: 큰 데이터셋에서 학습 시간이 오래 걸릴 수 있습니다.\n",
        "- 파라미터 튜닝 필요: 최적의 성능을 위해 커널 및 기타 매개변수 조정 필요\n",
        "\n",
        "**NDCG 개선 방법**:\n",
        "- 최적의 커널 함수 선택 (예: RBF, 폴리노미얼)\n",
        "- 매개변수 조정 (C 및 gamma 값 조정)\n",
        "- 특성 스케일링 (StandardScaler 사용)\n",
        "\n",
        "### 4. 그래디언트 부스팅 (Gradient Boosting)\n",
        "**장점**:\n",
        "- 높은 성능: 많은 데이터에서 좋은 성능을 보입니다.\n",
        "- 과적합 조절: 학습률(learning rate)과 트리 개수를 통해 조절 가능\n",
        "- 특성 중요도 파악 가능\n",
        "\n",
        "**단점**:\n",
        "- 느린 학습: 많은 트리를 순차적으로 학습하므로 시간이 오래 걸릴 수 있습니다.\n",
        "- 파라미터 튜닝 필요: 최적의 성능을 위해 여러 매개변수 조정 필요\n",
        "\n",
        "**NDCG 개선 방법**:\n",
        "- 더 많은 트리 사용 (n_estimators 증가)\n",
        "- 학습률 조정 (learning_rate 조정)\n",
        "- 트리 깊이 조정 (max_depth 조정)\n",
        "- 부스팅 유형 변경 (XGBoost 또는 LightGBM 사용)\n"
      ],
      "metadata": {
        "id": "pzZ0LscZfUqY"
      }
    }
  ]
}